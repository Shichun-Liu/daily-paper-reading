{"model（基于规则）；":{"model（基于规则）；":{"currentFile":{"count":1,"lastUpdated":1702287648155}}},"下面还计算了理论的sequence-level的分布误差的上界；（虽然我们更关心）":{"下面还计算了理论的sequence-level的分布误差的上界；（虽然我们更关心）":{"currentFile":{"count":1,"lastUpdated":1702304874167}}},"\\theta":{"\\theta":{"currentFile":{"count":1,"lastUpdated":1702305397824}}},"guidance":{"guidance":{"currentFile":{"count":1,"lastUpdated":1702306680473}}},"MODELS":{"MODELS":{"currentFile":{"count":1,"lastUpdated":1702361465902}}},"通过模型融合的方式，把不常被使用的专家模型融合在一起，提高参数效率和减小存储占用；":{"通过模型融合的方式，把不常被使用的专家模型融合在一起，提高参数效率和减小存储占用；":{"currentFile":{"count":1,"lastUpdated":1702361930472}}},"Model Merging in Language Models":{"Model Merging in Language Models":{"internalLink":{"count":2,"lastUpdated":1702953195025}}},"Oracle":{"Oracle":{"currentFile":{"count":1,"lastUpdated":1702477429788}}},"2205.14219":{"2205.14219":{"currentFile":{"count":1,"lastUpdated":1702477442660}}},"vector难以选择；并不鲁棒；":{"vector难以选择；并不鲁棒；":{"currentFile":{"count":1,"lastUpdated":1702695384200}}},"Q-learning":{"Q-learning":{"currentFile":{"count":1,"lastUpdated":1702696439446}}},"function进行变换；":{"function进行变换；":{"currentFile":{"count":1,"lastUpdated":1702696646444}}},"metrics；":{"metrics；":{"currentFile":{"count":1,"lastUpdated":1702709152584}}},"Metrics；":{"Metrics；":{"currentFile":{"count":1,"lastUpdated":1702709209680}}},"metrics":{"metrics":{"currentFile":{"count":1,"lastUpdated":1702713608588}}},"gradient中；":{"gradient中；":{"currentFile":{"count":1,"lastUpdated":1702728538715}}},"actor-crtic；":{"actor-crtic；":{"currentFile":{"count":1,"lastUpdated":1702733304775}}},"重要性采样":{"重要性采样":{"currentFile":{"count":1,"lastUpdated":1702736413086}}},"显著优于模仿学习，二元判别通常表现和缩放与模仿学习非常相似。并提出一个：LM":{"显著优于模仿学习，二元判别通常表现和缩放与模仿学习非常相似。并提出一个：LM":{"currentFile":{"count":1,"lastUpdated":1702795511790}}},"整体流程：以预训练模型为起点（PLM），往右根据互联网的":{"整体流程：以预训练模型为起点（PLM），往右根据互联网的":{"currentFile":{"count":1,"lastUpdated":1702807587308}}},"RLHF":{"RLHF":{"internalLink":{"count":2,"lastUpdated":1703313484357}}},"通过收集人类的偏好数据并应用偏好建模（PM）和从人类反馈中强化学习（RLHF）的技术来训练一个相对有帮助和无害（HH）的Agent；":{"通过收集人类的偏好数据并应用偏好建模（PM）和从人类反馈中强化学习（RLHF）的技术来训练一个相对有帮助和无害（HH）的Agent；":{"currentFile":{"count":1,"lastUpdated":1702816014790}}},"AlphaGo":{"AlphaGo":{"currentFile":{"count":1,"lastUpdated":1702819319084}}},"UCB公式\\-一种state-value的表达式":{"UCB公式\\-一种state-value的表达式":{"currentFile":{"count":1,"lastUpdated":1702822278573}}},"我们不讨论哲学问题，那么人类能否造出比自己":{"我们不讨论哲学问题，那么人类能否造出比自己":{"currentFile":{"count":1,"lastUpdated":1702867736168}}},"人类是如何完成数学推理和创新的？传统的机器学习+数学推导是如何进行的？":{"人类是如何完成数学推理和创新的？传统的机器学习+数学推导是如何进行的？":{"currentFile":{"count":1,"lastUpdated":1702868285321}}},"AlphaZero":{"AlphaZero":{"currentFile":{"count":1,"lastUpdated":1702868585158}}},"核心：尽量减少人类提供反馈数据，人类的唯一输入是宪法（一套规则），让另一个模型对其中的harmful内容进行检测并反馈；":{"核心：尽量减少人类提供反馈数据，人类的唯一输入是宪法（一套规则），让另一个模型对其中的harmful内容进行检测并反馈；":{"currentFile":{"count":1,"lastUpdated":1702952920166}}},"无害而有帮助的模型":{"无害而有帮助的模型":{"currentFile":{"count":1,"lastUpdated":1702959640341}}},"OpenAI":{"OpenAI":{"currentFile":{"count":1,"lastUpdated":1702964010001}}},"多模态大模型-2":{"多模态大模型-2":{"internalLink":{"count":1,"lastUpdated":1702964125876}}},"2021-12-09，Anthropic":{"2021-12-09，Anthropic":{"currentFile":{"count":1,"lastUpdated":1702968013251}}},"2022-03-10":{"2022-03-10":{"currentFile":{"count":1,"lastUpdated":1702968158813}}},"learning）和提示学习（prompt":{"learning）和提示学习（prompt":{"currentFile":{"count":1,"lastUpdated":1702972874734}}},"来源：OpenAI的Playground用户+数据标注工；":{"来源：OpenAI的Playground用户+数据标注工；":{"currentFile":{"count":1,"lastUpdated":1702973669579}}},"ChatGPT":{"ChatGPT":{"currentFile":{"count":1,"lastUpdated":1702976180394}}},"ELO算法的原理及应用":{"ELO算法的原理及应用":{"currentFile":{"count":3,"lastUpdated":1703311184044}}},"Bradley–Terry":{"Bradley–Terry":{"currentFile":{"count":1,"lastUpdated":1703312895567}}},"DPO——RLHF":{"DPO——RLHF":{"currentFile":{"count":1,"lastUpdated":1703318187113}}}}