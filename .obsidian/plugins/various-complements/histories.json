{"Model Merging in Language Models":{"Model Merging in Language Models":{"internalLink":{"count":4,"lastUpdated":1704386792729}}},"Oracle":{"Oracle":{"currentFile":{"count":1,"lastUpdated":1702477429788}}},"2205.14219":{"2205.14219":{"currentFile":{"count":1,"lastUpdated":1702477442660}}},"vector难以选择；并不鲁棒；":{"vector难以选择；并不鲁棒；":{"currentFile":{"count":1,"lastUpdated":1702695384200}}},"Q-learning":{"Q-learning":{"currentFile":{"count":1,"lastUpdated":1702696439446}}},"function进行变换；":{"function进行变换；":{"currentFile":{"count":1,"lastUpdated":1702696646444}}},"metrics；":{"metrics；":{"currentFile":{"count":1,"lastUpdated":1702709152584}}},"Metrics；":{"Metrics；":{"currentFile":{"count":1,"lastUpdated":1702709209680}}},"metrics":{"metrics":{"currentFile":{"count":1,"lastUpdated":1702713608588}}},"gradient中；":{"gradient中；":{"currentFile":{"count":1,"lastUpdated":1702728538715}}},"actor-crtic；":{"actor-crtic；":{"currentFile":{"count":1,"lastUpdated":1702733304775}}},"重要性采样":{"重要性采样":{"currentFile":{"count":1,"lastUpdated":1702736413086}}},"显著优于模仿学习，二元判别通常表现和缩放与模仿学习非常相似。并提出一个：LM":{"显著优于模仿学习，二元判别通常表现和缩放与模仿学习非常相似。并提出一个：LM":{"currentFile":{"count":1,"lastUpdated":1702795511790}}},"整体流程：以预训练模型为起点（PLM），往右根据互联网的":{"整体流程：以预训练模型为起点（PLM），往右根据互联网的":{"currentFile":{"count":1,"lastUpdated":1702807587308}}},"RLHF":{"RLHF":{"internalLink":{"count":2,"lastUpdated":1703313484357}}},"通过收集人类的偏好数据并应用偏好建模（PM）和从人类反馈中强化学习（RLHF）的技术来训练一个相对有帮助和无害（HH）的Agent；":{"通过收集人类的偏好数据并应用偏好建模（PM）和从人类反馈中强化学习（RLHF）的技术来训练一个相对有帮助和无害（HH）的Agent；":{"currentFile":{"count":1,"lastUpdated":1702816014790}}},"AlphaGo":{"AlphaGo":{"currentFile":{"count":1,"lastUpdated":1702819319084}}},"UCB公式\\-一种state-value的表达式":{"UCB公式\\-一种state-value的表达式":{"currentFile":{"count":1,"lastUpdated":1702822278573}}},"我们不讨论哲学问题，那么人类能否造出比自己":{"我们不讨论哲学问题，那么人类能否造出比自己":{"currentFile":{"count":1,"lastUpdated":1702867736168}}},"人类是如何完成数学推理和创新的？传统的机器学习+数学推导是如何进行的？":{"人类是如何完成数学推理和创新的？传统的机器学习+数学推导是如何进行的？":{"currentFile":{"count":1,"lastUpdated":1702868285321}}},"AlphaZero":{"AlphaZero":{"currentFile":{"count":1,"lastUpdated":1702868585158}}},"核心：尽量减少人类提供反馈数据，人类的唯一输入是宪法（一套规则），让另一个模型对其中的harmful内容进行检测并反馈；":{"核心：尽量减少人类提供反馈数据，人类的唯一输入是宪法（一套规则），让另一个模型对其中的harmful内容进行检测并反馈；":{"currentFile":{"count":1,"lastUpdated":1702952920166}}},"无害而有帮助的模型":{"无害而有帮助的模型":{"currentFile":{"count":1,"lastUpdated":1702959640341}}},"OpenAI":{"OpenAI":{"currentFile":{"count":2,"lastUpdated":1704352186034}}},"多模态大模型-2":{"多模态大模型-2":{"internalLink":{"count":1,"lastUpdated":1702964125876}}},"2021-12-09，Anthropic":{"2021-12-09，Anthropic":{"currentFile":{"count":1,"lastUpdated":1702968013251}}},"2022-03-10":{"2022-03-10":{"currentFile":{"count":1,"lastUpdated":1702968158813}}},"learning）和提示学习（prompt":{"learning）和提示学习（prompt":{"currentFile":{"count":1,"lastUpdated":1702972874734}}},"来源：OpenAI的Playground用户+数据标注工；":{"来源：OpenAI的Playground用户+数据标注工；":{"currentFile":{"count":1,"lastUpdated":1702973669579}}},"ChatGPT":{"ChatGPT":{"currentFile":{"count":1,"lastUpdated":1702976180394}}},"ELO算法的原理及应用":{"ELO算法的原理及应用":{"currentFile":{"count":3,"lastUpdated":1703311184044}}},"Bradley–Terry":{"Bradley–Terry":{"currentFile":{"count":1,"lastUpdated":1703312895567}}},"DPO——RLHF":{"DPO——RLHF":{"currentFile":{"count":1,"lastUpdated":1703318187113}}},"{\\text{ref}}":{"{\\text{ref}}":{"currentFile":{"count":2,"lastUpdated":1703330615328}}},"约束的强度。我们的实验表明了这种加权的重要性，因为没有加权的朴素版本会导致语言模型的退化":{"约束的强度。我们的实验表明了这种加权的重要性，因为没有加权的朴素版本会导致语言模型的退化":{"currentFile":{"count":1,"lastUpdated":1703331002671}}},"Prompt":{"Prompt":{"currentFile":{"count":1,"lastUpdated":1703487067640}}},"scaling":{"scaling":{"currentFile":{"count":1,"lastUpdated":1703489540912}}},"scaling；":{"scaling；":{"currentFile":{"count":1,"lastUpdated":1703490659027}}},"LLM-basic":{"LLM-basic":{"internalLink":{"count":1,"lastUpdated":1703490892058}}},"models":{"models":{"currentFile":{"count":1,"lastUpdated":1703491881132}}},"FunSearch":{"FunSearch":{"currentFile":{"count":1,"lastUpdated":1703496566155},"internalLink":{"count":1,"lastUpdated":1703507023992}}},"Controllable Text Generation with Neurally-Decomposed Oracle":{"Controllable Text Generation with Neurally-Decomposed Oracle":{"internalLink":{"count":1,"lastUpdated":1703507128546}}},"embedding":{"embedding":{"currentFile":{"count":1,"lastUpdated":1703562506985}}},"Pre-Trained Language Models for Interactive Decision-Making":{"Pre-Trained Language Models for Interactive Decision-Making":{"internalLink":{"count":1,"lastUpdated":1703577610199}}},"在专家轨迹上微调；":{"在专家轨迹上微调；":{"currentFile":{"count":1,"lastUpdated":1703578145902}}},"function":{"function":{"currentFile":{"count":1,"lastUpdated":1704168338756}}},"使用PPO价值模型":{"使用PPO价值模型":{"currentFile":{"count":1,"lastUpdated":1704178576357}}},"action":{"action":{"currentFile":{"count":1,"lastUpdated":1704182913236}}},"Proficiency}":{"Proficiency}":{"currentFile":{"count":2,"lastUpdated":1704192856608}}},"世界模型由一个LLM充当，利用Prompting的方式进行交互；相当于GPT4给出中间reward信号；":{"世界模型由一个LLM充当，利用Prompting的方式进行交互；相当于GPT4给出中间reward信号；":{"currentFile":{"count":1,"lastUpdated":1704193475042}}},"行）。当遇到树中的叶节点时，MCTS":{"行）。当遇到树中的叶节点时，MCTS":{"currentFile":{"count":1,"lastUpdated":1704200373186}}},"Models":{"Models":{"currentFile":{"count":1,"lastUpdated":1704200910630}}},"collection":{"collection":{"currentFile":{"count":1,"lastUpdated":1704386489929}}},"policy":{"policy":{"currentFile":{"count":1,"lastUpdated":1704386863555}}},"领域：机器人操作；":{"领域：机器人操作；":{"currentFile":{"count":1,"lastUpdated":1704783383391}}},"Problem":{"Problem":{"currentFile":{"count":1,"lastUpdated":1705307552646}}},"最优（独立于它们是否与测试数据集中相同的计划）":{"最优（独立于它们是否与测试数据集中相同的计划）":{"currentFile":{"count":1,"lastUpdated":1705320455277}}},"Reasoning":{"Reasoning":{"currentFile":{"count":1,"lastUpdated":1705328835134}}}}