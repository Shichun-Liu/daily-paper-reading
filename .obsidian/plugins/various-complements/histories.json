{"Model Merging in Language Models":{"Model Merging in Language Models":{"internalLink":{"count":4,"lastUpdated":1704386792729}}},"RLHF":{"RLHF":{"internalLink":{"count":2,"lastUpdated":1703313484357}}},"OpenAI":{"OpenAI":{"currentFile":{"count":2,"lastUpdated":1704352186034}}},"ELO算法的原理及应用":{"ELO算法的原理及应用":{"currentFile":{"count":3,"lastUpdated":1703311184044}}},"Bradley–Terry":{"Bradley–Terry":{"currentFile":{"count":1,"lastUpdated":1703312895567}}},"DPO——RLHF":{"DPO——RLHF":{"currentFile":{"count":1,"lastUpdated":1703318187113}}},"{\\text{ref}}":{"{\\text{ref}}":{"currentFile":{"count":2,"lastUpdated":1703330615328}}},"约束的强度。我们的实验表明了这种加权的重要性，因为没有加权的朴素版本会导致语言模型的退化":{"约束的强度。我们的实验表明了这种加权的重要性，因为没有加权的朴素版本会导致语言模型的退化":{"currentFile":{"count":1,"lastUpdated":1703331002671}}},"Prompt":{"Prompt":{"currentFile":{"count":1,"lastUpdated":1703487067640}}},"scaling":{"scaling":{"currentFile":{"count":1,"lastUpdated":1703489540912}}},"scaling；":{"scaling；":{"currentFile":{"count":1,"lastUpdated":1703490659027}}},"LLM-basic":{"LLM-basic":{"internalLink":{"count":1,"lastUpdated":1703490892058}}},"models":{"models":{"currentFile":{"count":1,"lastUpdated":1703491881132}}},"FunSearch":{"FunSearch":{"currentFile":{"count":1,"lastUpdated":1703496566155},"internalLink":{"count":1,"lastUpdated":1703507023992}}},"Controllable Text Generation with Neurally-Decomposed Oracle":{"Controllable Text Generation with Neurally-Decomposed Oracle":{"internalLink":{"count":1,"lastUpdated":1703507128546}}},"embedding":{"embedding":{"currentFile":{"count":1,"lastUpdated":1703562506985}}},"Pre-Trained Language Models for Interactive Decision-Making":{"Pre-Trained Language Models for Interactive Decision-Making":{"internalLink":{"count":1,"lastUpdated":1703577610199}}},"在专家轨迹上微调；":{"在专家轨迹上微调；":{"currentFile":{"count":1,"lastUpdated":1703578145902}}},"function":{"function":{"currentFile":{"count":1,"lastUpdated":1704168338756}}},"使用PPO价值模型":{"使用PPO价值模型":{"currentFile":{"count":1,"lastUpdated":1704178576357}}},"action":{"action":{"currentFile":{"count":1,"lastUpdated":1704182913236}}},"Proficiency}":{"Proficiency}":{"currentFile":{"count":2,"lastUpdated":1704192856608}}},"世界模型由一个LLM充当，利用Prompting的方式进行交互；相当于GPT4给出中间reward信号；":{"世界模型由一个LLM充当，利用Prompting的方式进行交互；相当于GPT4给出中间reward信号；":{"currentFile":{"count":1,"lastUpdated":1704193475042}}},"行）。当遇到树中的叶节点时，MCTS":{"行）。当遇到树中的叶节点时，MCTS":{"currentFile":{"count":1,"lastUpdated":1704200373186}}},"Models":{"Models":{"currentFile":{"count":1,"lastUpdated":1704200910630}}},"collection":{"collection":{"currentFile":{"count":1,"lastUpdated":1704386489929}}},"policy":{"policy":{"currentFile":{"count":1,"lastUpdated":1704386863555}}},"领域：机器人操作；":{"领域：机器人操作；":{"currentFile":{"count":1,"lastUpdated":1704783383391}}},"Problem":{"Problem":{"currentFile":{"count":1,"lastUpdated":1705307552646}}},"最优（独立于它们是否与测试数据集中相同的计划）":{"最优（独立于它们是否与测试数据集中相同的计划）":{"currentFile":{"count":1,"lastUpdated":1705320455277}}},"Reasoning":{"Reasoning":{"currentFile":{"count":1,"lastUpdated":1705328835134}}},"Agent论文分享：将RL引入LLM":{"Agent论文分享：将RL引入LLM":{"currentFile":{"count":1,"lastUpdated":1705752313357}}},"cube；":{"cube；":{"currentFile":{"count":1,"lastUpdated":1705757644735}}},"方法，该方法以累积和迭代的方式使用语言模型来模拟人类思维过程。通过将任务分解为更小的组件":{"方法，该方法以累积和迭代的方式使用语言模型来模拟人类思维过程。通过将任务分解为更小的组件":{"currentFile":{"count":1,"lastUpdated":1705760779908}}},"MathPrompter：Mathematical Reasoning using Large Language Models":{"MathPrompter：Mathematical Reasoning using Large Language Models":{"internalLink":{"count":1,"lastUpdated":1705822461215}}},"Games":{"Games":{"currentFile":{"count":1,"lastUpdated":1705843907473}}},"Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game":{"Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game":{"internalLink":{"count":1,"lastUpdated":1705848898936}}},"ReFT：Reasoning with Reinforced Fine-Tuning":{"ReFT：Reasoning with Reinforced Fine-Tuning":{"internalLink":{"count":1,"lastUpdated":1705912409435}}},"prompts":{"prompts":{"currentFile":{"count":1,"lastUpdated":1706080917135}}}}