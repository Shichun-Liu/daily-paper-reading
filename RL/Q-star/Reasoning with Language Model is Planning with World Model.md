---
tags:
  - MCTS
  - decision
date: 2023-10-23
---
[【论文解析】增强LLM复杂任务推理能力 - 知乎](https://zhuanlan.zhihu.com/p/636625781)
- 对于三个任务场景中不同的state和action定义讲得清楚；

## 摘要
- LLM 仍然难以解决对人类来说很容易的问题，例如为制定行动计划，或者做复杂的数学题、逻辑和常识推理。造成这种缺陷的原因是 LLM 缺乏内部**世界模型**来预测**世界状态**（例如，环境状态、中间变量值）和**模拟行动的长期结果**，阻碍了 LLM 执行类似于人脑的深思熟虑的计划。
- 为了克服LLM的缺陷，本文提出了一个 LLM 推理框架，即 **Reasoning via Planning (RAP)**。 RAP 将 LLM 重新定位为既扮演世界模型又进行推理的角色，并结合了MCTS算法，在庞大的的推理空间中进行搜索。在推理过程中，LLM（作为推理者）在 LLM（作为世界模型）和任务特定奖励的指导下逐步构建推理树，并在广度与深度之间取得适当的平衡，从而有效地获得高奖励推理路径。
- 任务：计划生成，GSM8k，逻辑推演PrOntoQA；
- 世界模型由一个LLM充当，利用Prompting的方式进行交互；相当于把中间状态说出来，用另一个LLM给出中间reward信号，再和跟最终目标的“距离”进行线性组合（后面会介绍r1和r2）；

![image.png](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20240102194426.png)

## RAP
### 世界模型
- 与具体任务相关，主要是要定义state和action；把推理过程描述为MDP；
- 使用（预测的）世界状态增强推理有助于 LLM 具有更接地和连贯的推理。请注意，完整的推理跟踪由 LLM 本身（作为具有内部世界模型的推理代理）模拟，而无需与外部真实环境交互。
### reward model
- 每个action 的评估由reward function给出；具体而言可能有以下几种形式：
	- **Likelihood of the action**（将选择该action的logit作为reward，反应的是”**快思考**“）. When an action is generated by the LLM conditioning on the in-context demonstration and the current state, the probability of the specific action reflects the LLM's preference. We thus can incorporate **the log probability of the action as a reward**. This reward reflects the "instinct" of LLMs as an agent, and can be also used as a prior for which action to explore.
	- **Confidence of the state**（对于状态出现的是否常见作为合理性的判据）. State prediction is nontrivial in some problems, e.g., in math reasoning (Figure 2, middle), given an action (i.e., a subquestion), the world model predicts the next state by answering the subquestion. We incorporate the confidence of the state (i.e., answers in this case) as a reward. Specifically, we draw multiple sample answers from the world model, and use **the proportion of the most frequent answer as the confidence**. Higher confidence indicates that the state prediction is more consistent with the world knowledge of LLMs (Hao et al., 2023b), which typically leads to a more reliable reasoning step.
	- **Self-evaluation by the LLM**. LLM用问题“这个推理步骤是否正确？”批评自己是有益的，并使用“yes”的概率作为reward。
	- **Task-specific heuristics**：基于规则的方式给出reward；
- 使用了两个独立的奖励。首先，我们用一些示例测试用例及其解决方案提示LLM，然后计算给定当前状态的**动作的对数概率，记为r1**。该奖励反映了LLM作为推理代理的直觉。它通常表明，当目标只剩下几个步骤时，而对于远处的目标则不可靠。此外，我们在执行与目标的动作并提供奖励 r2 后比较新状态，并与满足的条件数量进行缩放（“特定于任务的**启发式**”奖励）。具体来说，当满足所有条件时，我们分配一个超大奖励以确保将选择该计划作为解决方案。
- 奖励的影响取决于**任务的性质**。例如，动作似然奖励对于计划生成至关重要，但对数学推理没有帮助。

### MCTS推理
- 对经典 MCTS 进行了一些修改：（1）对于开放域问题，例如数学问题，不可能枚举所有动作（子问题），因此我们通过从 LLM 中采样**固定数量**的潜在动作来减少动作空间，以当前状态和上下文演示的提示为条件。(2) 在选择阶段，如果之前没有访问的动作，我们使用轻量级局部奖励（例如自我评估奖励）估计 Q 值，然后使用 UCT 选择动作。这为探索提供了先验知识，这在有限的迭代预算下至关重要。
- **模拟**：为了估计预期的未来奖励 (Q 值)，该阶段使用世界模型模拟当前节点的未来情况。从上面的当前节点开始，在每个节点 s，我们用rollout policy创建一个动作，并使用世界模型来预测下一个状态。rollout过程一直持续到达到终端状态。有很多方法可以定义rollout policy（例如，通过添加不同的随机性）。在我们的实验中，为简单起见和减少的噪声，我们遵循与上述扩展类似的过程，即生成 d 个候选动作并选择最大局部奖励之一 a′ = maxa′ r(s, a)。在实践中，为了提高效率，我们放弃了计算成本高昂的 reward实现（例如，来自状态置信度的reward需要多次采样答案），并使用生成的 *轻量级* 奖励函数在模拟中选择动作。
- 回溯：更新Q-value；
- 一旦达到**预定数量的 MCTS 迭代**，我们终止算法并从构建的树中选择最终推理轨迹进行评估。选择有多种方法。一是从根节点开始，迭代选择Q值最高的动作，直到达到终端。此外，可以**直接从产生最高奖励的迭代中选择路径**，或者**选择访问最多的叶节点(以及各自的根到叶路径)**。在实践中，我们观察到第二种策略通常会产生最好的结果。

![image.png](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20240102192028.png)

## 其他
附录F对reward的形式选择进行了讨论；

![image.png|350](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20240102195752.png)

- 添加自我评估奖励可以进一步提高性能；
- 如表 6 所示的 GSM8k 的前 300 个样本的结果，我们可以看到**在置信度奖励（第 2 行）之上添加动作似然（第 3 行）或自我评估（第 1 行）** 可以提高仅使用置信度奖励（第 1 行）进行一次迭代的 RAP 性能，但动作似然奖励会**随着迭代次数而降低准确性**。自我评估奖励总体上会带来最佳性能。这表明自我评估奖励在探索之前将推理引导为有效且计算效率高的重要性。
### 选择reward依据
1. 对于一个**推理步骤简短且结构化**的问题，**动作可能性**可以非常有效。否则，它可能会受到不重要的标记的干扰并变得不可靠。例如，BlocksWorld 域中的单步通常遵循特定的模式（例如，PICK/PUT/STACK 块。)，呈现动作可能性指示性。然而，在数学领域，推理步骤用自然语言句子表达，允许更大的自由和潜在的引入噪声。
2. 对于在生成过程中更容易识别一些错误的问题，自我评估作为提高推理准确性的有用机制出现。在数学推理中，LLM 可能首先难以生成正确的推理步骤，但计算或逻辑错误的检测更可行。