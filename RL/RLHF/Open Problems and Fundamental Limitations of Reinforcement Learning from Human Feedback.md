[大模型RLHF的局限和解法 - 知乎](https://zhuanlan.zhihu.com/p/652917012)

RLHF在三个阶段（feedback collection, reward modeling, and policy optimization）可能遇到的问题

![image.png](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20240105000212.png)

## feedback collection 
总结而言：
- 人有偏见；sycophancy；
- 人很懒，会犯错；gaslighting；
- 数据质量/成本的权衡；
- 反馈类型局限性；

## reward model 
- 单一的奖励函数不能代表一个多元化的人类社会；
- 为一个不完美的奖励代理进行优化会导致reward hacking；
- 评估奖励模型是困难和昂贵的；

## policy 
- RL代理需要在探索新策略和利用已知的奖励之间找到平衡；
- 深度RL的结果对初始条件非常敏感，且难以复制；
- 策略可能受到对抗性攻击；
- 部署环境可能与训练和评估环境不同，因此即使在训练期间的奖励是准确的，策略在部署期间也可能表现不佳；策略可能会追求错误的目标。例如，一个经过RLHF训练的系统可能会追求奖励机制本身，而不是真正的目标。
- 预训练模型在策略优化中带入了偏见；
- 强化学习的微调减少了模型输出的样本多样性，导致“模式崩溃”；
- 联合训练导致数据分布的变化；平衡效率与策略的过拟合是困难的；

### **应对RLHF的问题**

针对人类反馈应对的挑战：

- 借助AI提供反馈
- 细节反馈：对特定示例部分或与不同目标的精确、详细反馈有助于完善奖励模型。
- 基于流程的指导
- 将自然语言描述转化为奖励模型
- 从人类示范中学习奖励（逆向强化学习）：相比于反馈，人类的示范更能教导奖励模型。

针对奖励模型的挑战：

- 采用直接的人类监督
- 多目标管理
- 对已学习的奖励功能保持不确定性

针对策略的挑战：

- 在预训练中调整LLMs
- 通过有监督的学习调整LLMs

**RLHF并不是唯一解决方案：为安全性提供的其他策略**

鲁棒性：

- 包括对抗性示例训练、使用异常检测技巧，以及确保AI训练不受恶意攻击。

风险评估与审计：

- 无论是否能完全证明安全性，内部和第二方的评估都是识别潜在风险和建立信任的关键。
- 发展红队策略对于增强鲁棒性至关重要。

模型的可解释性与编辑：

- 在可解释性和解释方面的进步可以验证模型的决策流程，并在不进行大量测试的情况下提高对其安全性的信心。
- 使用红队策略、**深入了解内部机制**，以及直接调整模型权重或激活，都有助于提高模型的真实性、修改事实知识或调整模型行为。


还得是要从PPO的原理出发，真正搞清楚影响各种因素的机制。