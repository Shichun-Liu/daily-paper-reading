[利用人类反馈的强化学习来训练一个有用和无害的助手 - 知乎](https://zhuanlan.zhihu.com/p/605133974)
[NLP重铸篇之LLM系列(Anthropic LLM) - 知乎](https://zhuanlan.zhihu.com/p/632233773)

> [!abstract]
> 通过收集人类的偏好数据并应用偏好建模（PM）和从人类反馈中强化学习（RLHF）的技术来训练一个相对有帮助和无害（HH）的Agent；
> 可以将对齐任务和特定的nlp任务混合起来训练，能够不影响对齐效果也不影响任务性能。
- [[#主要内容|主要内容]]
	- [[#主要内容#**整体流程**|整体流程]]
	- [[#主要内容#**主要结果**|主要结果]]
- [[#数据|数据]]
- [[#上下文蒸馏|上下文蒸馏]]
- [[#偏好模型|偏好模型]]
	- [[#偏好模型#结果|结果]]
- [[#RLHF|RLHF]]
	- [[#RLHF#PPO|PPO]]
	- [[#RLHF#在线迭代|在线迭代]]
	- [[#RLHF#多样性|多样性]]
- [[#其他|其他]]

## 主要内容
**流程图**
![image.png|625](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20231217173053.png)

### 整体流程
以预训练模型为起点（PLM），往右根据互联网的**对比数据**进行预训练，得到预训练偏好模型（PMP），然后再在人类返回的比较数据集上微调，得到偏好模型（PM）；再以PLM为起点，往下根据提示数据，将520亿参数的模型结果蒸馏给更小的模型（会独立训练多个不同参数量的模型，从1300万到520亿），这个模型会作为强化学习的初始策略模型，然后以PM模型作为奖励模型，基于PPO的方法进行强化学习训练；根据得到的强化学习策略模型，生成新的**对比数据**，人工标注后重新训练PM模型，然后再重新训练强化学习模型，如此迭代。
重要步骤：人类反馈数据收集，PM训练，RLHF训练；

**与InstructGPT区别**
- 模型更大；
- 没有SFT阶段，有一个上下文蒸馏阶段；
- 在RL阶段没有混合预训练的数据；
- online训练，模型可以不断迭代更新；

### 主要结果

![image.png|500](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20231217180536.png)

经过RLHF训练的模型比原模型效果普遍更好，且随着模型规模提高模型效果越好；“online”在线训练能让模型效果更好，如果只在有效性数据上进行训练,模型效果更好，但同时无害性也有一定下降，一定程度表明有效性跟无害性的对立关系。

## 数据
论文论证了有效性和无害性通常是**对立的**，所以有效性数据和无害性数据也是分开收集的。对于有效性数据收集任务，标注人员会判断哪个结果更有效，对于无害性数据收集任务，标注人员则会判断哪个结果更无害。数据主要包括了以下三类：
1. 核心的base数据集：数据来自基础语言模型（context-distilled LM，参数量是520亿），包括了4.4万**有效数据对**和4.2万条**无害数据对**。
2. RS数据集：通过拒绝采样模型搜集（该模型是在base数据集上训练得到的偏好模型，参数量是520亿），包含了5.2万的有效数据对和2千个无害数据对，
3. online数据集：来自于RLHF模型，在5周里，每周更新一次，包含2.2万有效数据对，不包含无害性数据。

## 上下文蒸馏
把原先52B的模型蒸馏为一系列小模型；
1. 准备prompt数据：一半源于预训练数据，一半源于StackExchange数据；后者的话把高赞回复作为assistant的回复；
2. 用52B模型作为teacher模型输出top 50的token机器索引（没理解），形成一个小的数据集；
3. 把小模型在该数据集上进行微调；

## 偏好模型
经过三步骤，每个阶段一个epoch（避免过拟合）
1. 在大规模预料上进行自回归预训练；
2. 偏好模型的预训练：使用StackExchange/Reddit/Wikipedia的混合对比数据；学习率是第一步骤的10%；
3. 在人类反馈数据上继续fine-tune，学习率是第一步的1%；

### 结果
- 大部分模型首轮对话的acc最高，多轮之后持续下降；
- 训练数据/模型scale越大，acc越高；
- 无害性能力对于模型规模的提升不敏感；

**给分偏低**
- 如果要求样本对中的每个PM得分都要超过一个阈值，那么PM的准确性将随着阈值的增大而**降低**，如下图所示。这表明PM模型对高分样本判断不置信，一定程度上是因为**缺少高质量高分样本对**导致的，所以论文增加了“online”学习阶段，“online”阶段可以提供更多的高分样本，用来重新训练PM模型。

![image.png|550](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20231217200530.png)

## RLHF 
主要步骤：
1. 准备**比较数据集**，训练PM，让PM给出各个回复的分数；
2. 根据之前准备的prompt数据，根据PM给出的分数训练一个RL policy（action就是根据每个prompt生成一个回复，由PM提供reward）；
主要思想就是根据PM引导policy往更好回复的方向优化；

### PPO
reward的给出，以及对应的实际含义（优劣的概率）

![image.png|550](https://raw.githubusercontent.com/Shichun-Liu/images-on-picgo/main/pics/20231217201441.png)

- RLHF在较高的PM分数下逐渐变得不那么鲁棒；
- 较大的偏好模型比较小的偏好模型更鲁棒；
- 有效性数据和无害性数据之间存在对立性。

### 在线迭代
为了缓解高分数据缺少的问题，论文提出online的RLHF：
1. 人工标注：对于一个训练过的RLHF policy，和另外一个policy一起生成比较数据，然后让人类进行标注，收集起来；
2. 新的标注数据混入旧数据，训练一个新的PM模型，然后让新的PM参与PPO训练；

### 多样性
- RLHF一定程度上减小了输出的多样性，也就降低了后续数据收集的多样性；
- 通过部署不同版本的RL模型/在线迭代的方式来解决；

## 其他
- 对齐对于模型能力的损害，随着模型规模的增大而减小；
- helpful和harmless在一定程度上**对立**；在计算总loss的时候通过一个超参$\lambda$ 进行控制；
- 提出了一些降低模型有害性输出的方法，如OOD（分布外检测）来拒绝回答有害请求；