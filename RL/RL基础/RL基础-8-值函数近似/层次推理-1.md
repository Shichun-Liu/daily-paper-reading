- [GitHub - AGI-Edgerunners/LLM-Planning-Papers: Must-read Papers on Large Language Model (LLM) Planning.](https://github.com/AGI-Edgerunners/LLM-Planning-Papers)
- [GitHub - atfortes/LLM-Reasoning-Papers: Collection of papers and resources on Reasoning in Large Language Models (LLMs), including Chain-of-Thought (CoT), Instruction-Tuning, and others.](https://github.com/atfortes/LLM-Reasoning-Papers)

## Hierarchies/Meta-learning

### RL+Hierarchies

- 传统的多层次RL训练通常被运用于**机器人**领域，负责**多个子任务**的学习与泛化；
- 元学习领域；
- [\[1710.09767\] Meta Learning Shared Hierarchies](https://arxiv.org/abs/1710.09767)
- [\[1812.00025\] Modulated Policy Hierarchies](https://arxiv.org/abs/1812.00025)
- [\[2002.05954\] Learning Functionally Decomposed Hierarchies for Continuous Control Tasks with Path Planning](https://arxiv.org/abs/2002.05954)
- \[\[Q-Transformer：Scalable Offline Reinforcement Learning via Autoregressive Q-Functions\]\]

能否把推理的整个过程，拆解成几个基本动作（比如推理预测/评估/验证）；
现有的算法通常是固定的若干个action以一定顺序依次执行；那么我们能否训练一个master policy，自主决策采取什么action；
- 训练LLM+RL master policy的层次policy；
- task-specific LLM分别处理辅助推理任务的action，比如reflexion；
- 这些动作之间的地位如何？平级的，还是有优先级？reward怎么定义？
- 任务选择什么？经典的planning任务还是math？math更具挑战性一些；

## Math

- math的困难在于根本没法（？）把规则全部表示出来然后使用专门的推理器（在符号逻辑/planning领域有专门的推理器），只能让LLM来完成基本的推理动作；
- 基于搜索的推理（不管是ToT-BFS/DFS/MCTS）都有可能找不到有效路径，因为生成节点的方向/准确性是不保证的（由LLM完成）；
- 两个极端：推理规则完全确定，好做；完全没有推理规则（或者说规则太过于庞杂）：数学问题，不好做；
- 围棋问题属于是完全基于搜索的算法，好在情景比较单一，没有推理可言就是纯搜索；
  \### 论文
- MathPrompter：Mathematical Reasoning using Large Language Models
  - \[\[MathPrompter：Mathematical Reasoning using Large Language Models\]\]
  - prompt engineering；
  - 设计了两种prompt（数学方程和Python函数）来引导语言模型进行"思考"，并且这两种结果可以交叉验证。一方面预设了中间步骤，同时通过带入不同的数值测试进一步避免了中间步骤正确但计算结果错误。
- Learning From Mistakes Makes LLM Better Reasoner
  - GSM8k/MATH；
  - 从错误中学习(LEMA)；
  - GPT-4生成的错误纠正数据对对llm进行微调。具体来说，我们首先从各种llm中收集不准确的推理路径，然后使用GPT-4作为"纠错器"来(1)识别错误步骤，(2)解释错误原因，(3)纠正错误并生成最终答案；
- Teaching Language Models to Self-Improve through Interactive Demonstrations
  - 和上一篇工作类似；
  - 一种训练算法 TRIPOST，它能更有效地训练一个小型模型，使其从错误中学习，产生反馈，并提高其在数学和推理任务中的表现。
  - TRIPOST 是一种迭代算法，由三个阶段组成： 主动轨迹编辑（Inter- active Trajectory Editing）、数据后处理（Data Post-processing）和模型训练（Model Training）。
  - 与强化学习中的探索阶段类似，TRIPOST 首先使用小模型创建改进示范，与专家 LLM 或相关 Python 脚本进行交互。然后，TRIPOST 对收集到的数据进行后处理，过滤掉失败的改进尝试，然后重新平衡数据集，使模型即使在尝试已经正确的情况下也不再尝试 "改进"。最后，TRIPOST 重放处理后的数据集。
- Improving Large Language Model Fine-tuning for Solving Math Problems
  - [论文分享：Improving Large Language Model Fine-tuning For Solving Math Problems - 知乎](https://zhuanlan.zhihu.com/p/667243909)
  - 作者指出这样一个现象：针对一个数学问题，PaLM2一次输出正确答案（Pass@1）的概率是很低的，但当其重复输出64次（在较高的温度设定下，Pass@64），命中正确答案的概率提升到67.8%。这就说明：**PaLM2有能力生成正确答案，但其无法判断生成的答案是否正确。**

## Planning/Reasoning

### 推理本身

- [LeCun引战，LLM根本不会推理！大模型「涌现」，终究离不开上下文学习](https://mp.weixin.qq.com/s/apNDE-I2MNpTmmaZI1cY4A)
- [\[2206.10498\] PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change](https://arxiv.org/abs/2206.10498)
- [Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) \| OpenReview](https://openreview.net/forum?id=wUU-7XTL5XO)
  - 本身不具备规划能力（现有的规划能力只是因为任务比较简单通过ICL可以给出答案）；
- [\[2305.16151\] Understanding the Capabilities of Large Language Models for Automated Planning](https://arxiv.org/abs/2305.16151)
  - planning的边界，定义，四个问题，但是结论比较显然，没什么insight；
  - 衡量：对 LLM 生成的计划与最佳计划使用基于动作的标准，即两个计划中动作顺序之间的汉明距离；
- LLM不懂**逻辑**；
- [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters - 知乎](https://zhuanlan.zhihu.com/p/661161200)指出，CoT示例，给的这些demonstration更多的不是去教LLM怎么去推理，而是给出了某种**output format/space**，告诉LLMs应该生成这样看上去一步一步的解，同时这些解需要跟原问题相关的，同时也要follow正常的顺序；但实际里面的推理**不需要是对的**，因此它没有真的学习，而是引出**预训练**中得到的推理能力。

### setting/motivation

- 从LLM训练过程、基本架构来看，其在进行文本生成的过程中，并不具备严谨推理所需的规则匹配、状态转移、搜索等要素；
- 其在现有任务上【表现】出来的推理能力源于预训练数据中存在类似的文本，所以本质上还是在ICL；
- 应当要扬长避短，不是让LLM直接做完整推理，而是让LLM参与推理之中的某一些步骤，提供一些过程信号；让LLM做某一步具体的工作，或者多个LLM作为Agent各司其职，模块化整个推理流程；
- 使用PDDL进行推理的基本动作可以保证推理过程的准确（只要输入的文件是准确的）；
- math并不能简单使用PDDL解决，因为数学代数rules非常多，人类在推理的时候是隐式遵循了非常多的规则；math问题现在还是只能让LLM进行最基本的推理步骤，所以准确率很低；

### LLM+PDDL+Planning

- \[\[LLM+P：Empowering Large Language Models with Optimal Planning Proficiency\]\]
  - 任务：Robotic，7 domains；
  - 将经典规划器的优势纳入 LLM 的框架；
  - LLM+P 通过首先将语言描述转换为以规划域定义语言 (PDDL) 文件，然后利用经典规划器快速找到解决方案，然后将找到的解决方案转换回自然语言；
  - LLM最强的能力是语言建模，多种序列之间的转化；实验结果比直接的LLM-as-Planner好很多；
- PDDL Planning with Pretrained Large Language Models
  - 任务：Pyperplan domain（18个）；原始benchmark数据可能被预训练过，因此实验修改了一部分；
- Plansformer: Generating Symbolic Plans using Transformers
  - 任务：Blocksworld, Towers of Hanoi, Grippers, Driverlog；
  - 微调 CodeT5 以解决规划语法和语义；
  - 利用在代码生成上预训练的LLM (CodeT5)，并进一步用相应的有效计划在规划问题实例上训练它。
  - 两种类型的测试来评估它在生成有效计划（或几乎有效的未见规划问题实例计划）方面的能力：1）如果 Plansformer 可以生成有意义的响应（如测试数据集中所示）模型测试度量，2）如果生成的计划有效/最优（独立于它们是否与测试数据集中相同的计划）
- Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models
  - 任务：Blocksworld；
  - 训练了一个验证器，它将生成的动作分类为给定状态下有效或无效；
- Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning
  - 任务：经典规划文献和家庭领域的两个IPC领域；通过手动评估来评估生成的 PDDL 模型的质量；
  - 在规划域定义语言 (PDDL) 中构建了一个显式world（domain）model，然后使用它计划域无关的规划器；
  - 考虑到LLM一开始可能无法生成无错误PDDL模型的事实。为了解决这个问题，我们表明 LLM 也可以作为 PDDL 和任何反馈源之间的接口，这些反馈源可以在自然语言中提供纠正反馈，例如 VAL \[18\] 中的人类和 PDDL 验证器。

### LLM+Reasoning

大多都是Actor-Critic范式；新一点的是和准确求解器结合；
- Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning.
- 将 LLM 与**符号求解器**整合在一起，以改进逻辑问题的解决。
- 首先利用 LLM 将自然语言问题转化为符号表述。然后，一个确定性的符号求解器对所表述的问题进行推理。我们还引入了一个自我完善模块，利用符号求解器的错误信息来修改符号化。
- 在五个逻辑推理数据集上演示了 LOGIC-LM 的有效性：ProofWriter、PrOntoQA、FOLIO、LogicalDe- duction 和 AR-LSAT。
- Chain-of-Verification Reduces Hallucination in Large Language Models
- 研究语言模型思考自己给出的答案，从而纠正自己的错误的能力。引入了**验证链**( Chain-of-Verification，CoVe )，一种在大型语言模型中通过考虑自身的响应并自我修正来减少幻觉的方法。
- Question Decomposition Improves the Faithfulness of Model-Generated Reasoning
- [衡量和改善大模型生成的推理的忠实度 - 知乎](https://zhuanlan.zhihu.com/p/648216748)
- **思维链分解和因子分解**。思维链分解类似于思维链提示，不同之处在于推理明确格式化为"子问题"和"子答案"的列表。因子分解也使用子问题和子答案，但是在**隔离的上下文中**提示模型回答子问题，以限制模型在一个上下文中进行全部推理时可能产生的有偏见推理量。
- 因子分解也可以缓解有偏见的推理问题，我们使用[Turpin et al. (2023)](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.04388)提出的实验进行测试。具体而言，当我们提示模型执行因子分解而不是思维链或思维链分解时，在有偏见的情境下，模型的性能下降较少。
- REFINER: Reasoning Feedback on Intermediate Representations
- 用于微调LM以在与提供自动反馈的评论模型交互时明确生成中间推理步骤；
