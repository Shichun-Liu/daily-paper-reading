[Solving olympiad geometry without human demonstrations | Nature](https://www.nature.com/articles/s41586-023-06747-5#Sec2)（已下载）
项目文档：[飞书 - 登录](https://fudannlp.feishu.cn/docx/AUNKdmFcboaP9exNx4Ic6BXInjc?from=from_copylink)

## **推理本身**

如何实现数学领域的严格LLM-based的推理；

- 使用LLM进行推理存在缺陷：LLM并不理解（也没有机制可以去解释）推理涉及到的逻辑归纳与演绎；
    
- 应当将LLM作为语言模块而使用，推理动作的执行应当使用严格的搜索算法/推理算法。
    

### **资料**

- [LeCun引战，LLM根本不会推理！大模型「涌现」，终究离不开上下文学习](https://mp.weixin.qq.com/s/apNDE-I2MNpTmmaZI1cY4A)
    
- [[2206.10498] PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change]([https://arxiv.org/abs/2206.10498](https://arxiv.org/abs/2206.10498))
    
- [Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) | OpenReview](https://openreview.net/forum?id=wUU-7XTL5XO)
    
    - 本身不具备规划能力（现有的规划能力只是因为任务比较简单通过ICL可以给出答案）；
        
- [[2305.16151] Understanding the Capabilities of Large Language Models for Automated Planning]([https://arxiv.org/abs/2305.16151](https://arxiv.org/abs/2305.16151))
    
    - planning的边界，定义，四个问题，但是结论比较显然，没什么insight；
        
    - 衡量：对 LLM 生成的计划与最佳计划使用基于动作的标准，即两个计划中动作顺序之间的汉明距离；
        
- [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters - 知乎](https://zhuanlan.zhihu.com/p/661161200)指出，CoT示例不是去教LLM怎么去推理，而是给出了某种**output format/space**，告诉LLMs应该生成这样**看上去**一步一步的解，同时这些解需要跟原问题相关的、要follow正常的顺序；但实际里面的推理**不需要是对的**，因此它没有真的学习，而是引出**预训练**中得到的推理能力。
    

## 可靠推理

- 从LLM训练过程、基本架构来看，其在进行文本生成的过程中，并不具备严谨推理所需的规则匹配、状态转移、搜索等要素；其在现有任务上表现出来的推理能力源于预训练数据中存在类似的文本，所以本质上还是在ICL；因此，要扬长避短，不是让LLM直接推理/预测下一个动作，而是让LLM参与推理之中的某一些辅助动作，提供一些过程信号/自然语言到特定格式文本的翻译；
    

![](https://fudannlp.feishu.cn/space/api/box/stream/download/asynccode/?code=NzEyZTJiMzM4MjNiNzM4MzFiZmM3ZWU2MDQ4ZjM5MjJfM0JIMUlPV3RScFY0d1RIekpaQndYMlVmR3pvMkdXQ2pfVG9rZW46SDgxVWIyVlk5b1ZYbDh4eUVpcmN1a2I0bnBiXzE3MDU3NTY0Nzc6MTcwNTc2MDA3N19WNA)

- 使用PDDL进行推理动作可以保证逻辑推理过程的准确（只要输入的文件是准确的）；
    
- math并不能简单使用PDDL解决，因为数学代数rules非常多，人类在推理的时候是隐式遵循了非常多的规则；math问题现在还是只能让LLM进行最基本的推理步骤，所以准确率很低；
    
- [SatLM: Satisfiability-Aided Language Models Using Declarative Prompting](https://arxiv.org/abs/2305.09656)：LLM+SAT推理器解决代数问题；在 GSM 算术推理数据集的挑战性子集上比程序辅助 LM 高出 23%，在 LSAT 和 BOARDGAMEQA 上也实现了新的 SoTA；
    
- [Pal: Program-aided language models](https://arxiv.org/abs/2211.10435)：LLM+Python解决代数问题；
    
- [Faithful Chain-of-Thought Reasoning](https://arxiv.org/abs/2301.13379)：LLM+python/datalog/PDDL 解决代数、多跳问答、planning（robotic）、人际关系推理问题；
    

### **LLM+PDDL+Planning**

PDDL的优势在于这是一个强大的外部规划器，输入两个文件用来描述环境和目标，PDDL可以给出最优规划；LLM的工作就是翻译，把NL情景理解并转化为PDDL语言、把输出转化为NL；缺点是适配的任务有限、简单；

- [[LLM+P：Empowering Large Language Models with Optimal Planning Proficiency]]
    
    - 任务：Robotic，7 domains；
        
    - 将经典规划器的优势纳入 LLM 的框架；
        
    - LLM+P 通过首先将语言描述转换为以规划域定义语言 (PDDL) 文件，然后利用经典规划器快速找到解决方案，然后将找到的解决方案转换回自然语言；
        
    - LLM最强的能力是语言建模，多种序列之间的转化；实验结果比直接的LLM-as-Planner好很多；
        
- PDDL Planning with Pretrained Large Language Models
    
    - 任务：Pyperplan domain（18个）；原始benchmark数据可能被预训练过，因此实验修改了一部分；
        
- Plansformer: Generating Symbolic Plans using Transformers
    
    - 任务：Blocksworld, Towers of Hanoi, Grippers, Driverlog；
        
    - 微调 CodeT5 以解决规划语法和语义；
        
    - 利用在代码生成上预训练的LLM (CodeT5)，并进一步用相应的有效计划在规划问题实例上训练它。
        
    - 两种类型的测试来评估它在生成有效计划（或几乎有效的未见规划问题实例计划）方面的能力：1）如果 Plansformer 可以生成有意义的响应（如测试数据集中所示）模型测试度量，2）如果生成的计划有效/最优（独立于它们是否与测试数据集中相同的计划）
        
- Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models
    
    - 任务：Blocksworld；
        
    - 训练了一个验证器，它将生成的动作分类为给定状态下有效或无效；
        
- Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning
    
    - 任务：经典规划文献和家庭领域的两个IPC领域；通过手动评估来评估生成的 PDDL 模型的质量；
        
    - 在规划域定义语言 (PDDL) 中构建了一个显式world（domain）model，然后使用它计划域无关的规划器；
        
    - 考虑到LLM一开始可能无法生成无错误PDDL模型的事实。为了解决这个问题，我们表明 LLM 也可以作为 PDDL 和任何反馈源之间的接口，这些反馈源可以在自然语言中提供纠正反馈，例如 VAL [18] 中的人类和 PDDL 验证器。