## 传统层次推理/元学习
### RL+Hierarchies
- 传统的多层次RL训练通常被运用于**机器人**领域，负责多个子任务的学习与泛化；
- 元学习领域；
- [[1710.09767] Meta Learning Shared Hierarchies](https://arxiv.org/abs/1710.09767)
- [[1812.00025] Modulated Policy Hierarchies](https://arxiv.org/abs/1812.00025)
- [[2002.05954] Learning Functionally Decomposed Hierarchies for Continuous Control Tasks with Path Planning](https://arxiv.org/abs/2002.05954)
- [[Q-Transformer：Scalable Offline Reinforcement Learning via Autoregressive Q-Functions]]

- 训练LLM+RL master policy的层次policy；
	- task-specific LLM分别处理辅助推理任务的action，比如react，reflection；
	- 现有的算法通常是固定的若干个action以一定顺序依次执行；那么我们能否训练一个master policy，自主决策采取什么action；

## Math
- [x] [[MathPrompter：Mathematical Reasoning using Large Language Models]]
	- prompt engineering;
	- 设计了两种prompt（数学方程和Python函数）来引导语言模型进行“思考“，并且这两种结果可以交叉验证。一方面预设了中间步骤，同时通过带入不同的数值测试进一步避免了中间步骤正确但计算结果错误。
- 

## Planning
**资料**
- [GitHub - AGI-Edgerunners/LLM-Planning-Papers: Must-read Papers on Large Language Model (LLM) Planning.](https://github.com/AGI-Edgerunners/LLM-Planning-Papers)
- [GitHub - atfortes/LLM-Reasoning-Papers: Collection of papers and resources on Reasoning in Large Language Models (LLMs), including Chain-of-Thought (CoT), Instruction-Tuning, and others.](https://github.com/atfortes/LLM-Reasoning-Papers)

### LLM是否具备推理能力？
- [LeCun引战，LLM根本不会推理！大模型「涌现」，终究离不开上下文学习](https://mp.weixin.qq.com/s/apNDE-I2MNpTmmaZI1cY4A)
- [[2206.10498] PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change](https://arxiv.org/abs/2206.10498)
- [Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) | OpenReview](https://openreview.net/forum?id=wUU-7XTL5XO)
	- 本身不具备规划能力（现有的规划能力只是因为任务比较简单通过ICL可以给出答案）；
- [[2305.16151] Understanding the Capabilities of Large Language Models for Automated Planning](https://arxiv.org/abs/2305.16151)
	- planning的边界，定义，四个问题，但是结论比较显然；
	- 衡量：对 LLM 生成的计划与最佳计划使用基于动作的标准，即两个计划中动作顺序之间的汉明距离；

### 我的观点
- 从LLM训练过程、基本架构来看，其在进行文本生成的过程中，并不具备推理所需的规则匹配、状态转移、搜索等要素；
- 其在现有任务上【表现】出来的推理能力源于预训练数据中存在类似的文本，所以本质上还是在ICL；
- 应当要扬长避短，不是让LLM直接做完整推理，而是让LLM参与推理之中的某一些步骤，提供一些过程信号；让LLM做某一步具体的工作，或者多个LLM作为Agent各司其职，模块化整个推理流程；

### LLM+PDDL
- [x] [[LLM+P：Empowering Large Language Models with Optimal Planning Proficiency]]
	- Robotic；
	- 将经典规划器的优势纳入 LLM 的框架；
	- LLM+P 通过首先将语言描述转换为以规划域定义语言 (PDDL) 文件，然后利用经典规划器快速找到解决方案，然后将找到的解决方案转换回自然语言；
	- LLM最强的能力是语言建模，多种序列之间的转化；实验结果比直接的LLM-as-Planner好很多；
- PDDL Planning with Pretrained Large Language Models
	- Pyperplan domain（18个任务）；原始benchmark数据可能被预训练过，因此实验修改了一部分；
- Plansformer: Generating Symbolic Plans using Transformers
	- 