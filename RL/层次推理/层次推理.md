## related work
### RL+Hierarchies
- 传统的多层次RL训练通常被运用于**机器人**领域，负责多个子任务的学习与泛化；
- 元学习领域；
- [[1710.09767] Meta Learning Shared Hierarchies](https://arxiv.org/abs/1710.09767)
- [[1812.00025] Modulated Policy Hierarchies](https://arxiv.org/abs/1812.00025)
- [[2002.05954] Learning Functionally Decomposed Hierarchies for Continuous Control Tasks with Path Planning](https://arxiv.org/abs/2002.05954)
- [[Q-Transformer：Scalable Offline Reinforcement Learning via Autoregressive Q-Functions]]

- 训练LLM+RL master policy的层次policy；
	- task-specific LLM分别处理辅助推理任务的action，比如react，reflection；
	- 现有的算法通常是固定的若干个action以一定顺序依次执行；那么我们能否训练一个master policy，自主决策采取什么action；


## Planning
- [LeCun引战，LLM根本不会推理！大模型「涌现」，终究离不开上下文学习](https://mp.weixin.qq.com/s/apNDE-I2MNpTmmaZI1cY4A)
- [[2206.10498] PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change](https://arxiv.org/abs/2206.10498)
- [Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) | OpenReview](https://openreview.net/forum?id=wUU-7XTL5XO)

- [ ] LLM+P: Empowering Large Language Models with Optimal Planning Proficiency
	- 将经典规划器的优势纳入 LLM 的框架；
	- LLM+P 通过首先将语言描述转换为以规划域定义语言 (PDDL) 文件，然后利用经典规划器快速找到解决方案，然后将找到的解决方案转换回自然语言；
	- LLM最强的能力是语言建模，多种序列之间的转化；本身不具备规划能力（现有的规划能力只是因为任务比较简单通过ICL可以给出答案）；